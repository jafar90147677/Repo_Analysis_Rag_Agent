"""In-memory tracker for repo-scoped indexing concurrency state."""

from __future__ import annotations

import os
import threading
import time
from typing import Dict

from . import scan_rules
from .scan_rules import compute_repo_id
from .manifest_store import ManifestStore
from ..logging.logger import logger


_repo_indexing_state: Dict[str, bool] = {}
_repo_indexed_files: Dict[str, int] = {}
_repo_indexing_state_lock = threading.Lock()

class _RepoIndexingState:
    """Process-global, repo-keyed state for in-progress indexing."""

    @classmethod
    def mark_started(cls, repo_id: str) -> None:
        with _repo_indexing_state_lock:
            _repo_indexing_state[repo_id] = True
            _repo_indexed_files[repo_id] = 0
        logger.info("indexing_started repo_id=%s", repo_id)

    @classmethod
    def mark_finished(cls, repo_id: str) -> None:
        with _repo_indexing_state_lock:
            _repo_indexing_state.pop(repo_id, None)
        logger.info("indexing_finished repo_id=%s", repo_id)

    @classmethod
    def is_in_progress(cls, repo_id: str) -> bool:
        with _repo_indexing_state_lock:
            return repo_id in _repo_indexing_state

    @classmethod
    def has_active(cls) -> bool:
        with _repo_indexing_state_lock:
            return bool(_repo_indexing_state)


_repo_locks: Dict[str, threading.Lock] = {}
_repo_locks_lock = threading.Lock()

_indexed_files_so_far: int = 0
_last_index_completed_epoch_ms: int = 0
_health_tracking_lock = threading.Lock()
_pending_health_snapshot: bool = False


def _get_repo_lock(repo_id: str) -> threading.Lock:
    """Return the lock for `repo_id`, creating it if necessary."""
    with _repo_locks_lock:
        lock = _repo_locks.get(repo_id)
        if lock is None:
            lock = threading.Lock()
            _repo_locks[repo_id] = lock
        return lock


def mark_indexing_started(repo_id: str) -> None:
    """Record that indexing has started for `repo_id`."""
    _RepoIndexingState.mark_started(repo_id)
    with _health_tracking_lock:
        global _indexed_files_so_far
        global _last_index_completed_epoch_ms
        global _pending_health_snapshot
        _indexed_files_so_far = 0
        _last_index_completed_epoch_ms = 0
        _pending_health_snapshot = False


def mark_indexing_finished(repo_id: str) -> None:
    """Record that indexing has finished for `repo_id`."""
    _RepoIndexingState.mark_finished(repo_id)
    with _health_tracking_lock:
        global _last_index_completed_epoch_ms
        global _pending_health_snapshot
        _last_index_completed_epoch_ms = int(time.time() * 1000)
        _pending_health_snapshot = True


def increment_indexed_files(repo_id: str | None = None) -> None:
    """Increment the count of indexed files."""
    if repo_id:
        with _repo_indexing_state_lock:
            _repo_indexed_files[repo_id] = _repo_indexed_files.get(repo_id, 0) + 1
    with _health_tracking_lock:
        global _indexed_files_so_far
        _indexed_files_so_far += 1


def clear_health_snapshot() -> None:
    """Clear stored health snapshot values."""
    with _health_tracking_lock:
        global _indexed_files_so_far
        global _last_index_completed_epoch_ms
        global _pending_health_snapshot
        _indexed_files_so_far = 0
        _last_index_completed_epoch_ms = 0
        _pending_health_snapshot = False


def is_indexing_in_progress(repo_id: str) -> bool:
    """Return whether indexing is currently in progress for `repo_id`."""
    return _RepoIndexingState.is_in_progress(repo_id)


def is_any_indexing_in_progress() -> bool:
    """Return whether any repo is currently being indexed."""
    return _RepoIndexingState.has_active()


def get_health_stats(repo_id: str | None = None) -> dict:
    """Return the current indexing health statistics."""
    with _repo_indexing_state_lock:
        if repo_id:
            is_indexing = _repo_indexing_state.get(repo_id, False)
            indexed_files = _repo_indexed_files.get(repo_id, 0)
        else:
            is_indexing = any(_repo_indexing_state.values())
            indexed_files = _indexed_files_so_far
    with _health_tracking_lock:
        last_completed = _last_index_completed_epoch_ms
        return {
            "indexing": is_indexing,
            "indexed_files_so_far": indexed_files,
            "last_index_completed_epoch_ms": last_completed,
            "pending_snapshot": _pending_health_snapshot,
        }


def reset_indexing_stats() -> None:
    """Reset all indexing statistics and states (primarily for testing)."""
    with _repo_indexing_state_lock:
        _repo_indexing_state.clear()
        _repo_indexed_files.clear()
    with _health_tracking_lock:
        global _indexed_files_so_far, _last_index_completed_epoch_ms, _pending_health_snapshot
        _indexed_files_so_far = 0
        _last_index_completed_epoch_ms = 0
        _pending_health_snapshot = False


def reset_indexer_state() -> None:
    """Alias for reset_indexing_stats, used for tests/app startup."""
    reset_indexing_stats()


class RepoIndexingLock:
    """Context manager that acquires a repo-specific lock."""

    def __init__(self, repo_id: str) -> None:
        self.repo_id = repo_id
        self._lock = _get_repo_lock(repo_id)
        self._acquired = False

    def acquire(self, blocking: bool = True) -> bool:
        acquired = self._lock.acquire(blocking=blocking)
        if acquired:
            self._acquired = True
            mark_indexing_started(self.repo_id)
        return acquired

    def release(self) -> None:
        if not self._acquired:
            return
        try:
            mark_indexing_finished(self.repo_id)
        finally:
            self._lock.release()
            self._acquired = False

    def __enter__(self) -> "RepoIndexingLock":
        if not self.acquire(blocking=True):
            raise RuntimeError("Failed to acquire repo indexing lock")
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb) -> None:
        self.release()


def acquire_indexing_lock(repo_id: str) -> RepoIndexingLock:
    """Return a context manager that holds the repo lock for `repo_id`."""
    return RepoIndexingLock(repo_id)


def perform_indexing_scan(root_path: str, repo_id: str) -> dict:
    """
    Traverse the file system starting from root_path, enforcing boundaries and exclusions.
    """
    indexed_files = 0
    skipped_files = 0
    start_time = time.time()

    # Initialize manifest store
    manifest_path = scan_rules.index_dir() / repo_id / "manifest.json"
    manifest = ManifestStore(str(manifest_path))

    # Use os.walk to traverse the directory tree
    for root, dirs, files in os.walk(root_path):
        # 1. Symlink/Junction Detection (Precedence): Skip symlinks before any other rules
        # For directories: Modify 'dirs' in-place to prevent traversal
        for d in list(dirs):
            dir_path = os.path.join(root, d)
            if scan_rules.is_symlink_entry(dir_path):
                dirs.remove(d)
                # Record symlink in manifest
                manifest.add_symlink_entry(scan_rules.normalize_root_path(dir_path))
                continue
            
            # 2. Directory Exclusion: Skip excluded directories
            if scan_rules.should_skip_path(root_path, dir_path, is_dir=True):
                dirs.remove(d)
                skipped_files += 1

        # 3. File Exclusion: Process files in the current directory
        for f in files:
            file_path = os.path.join(root, f)
            # Symlink check takes precedence
            if scan_rules.is_symlink_entry(file_path):
                # Record symlink in manifest
                manifest.add_symlink_entry(scan_rules.normalize_root_path(file_path))
                continue
                
            if scan_rules.should_skip_path(root_path, file_path, is_dir=False):
                skipped_files += 1
            else:
                # Binary detection: skip if first 4096 bytes contain a null byte
                try:
                    with open(file_path, "rb") as bf:
                        chunk = bf.read(4096)
                        if b"\x00" in chunk:
                            mtime_ms = int(os.path.getmtime(file_path) * 1000)
                            indexed_at_ms = int(time.time() * 1000)
                            manifest.add_entry(
                                scan_rules.normalize_root_path(file_path), 
                                status="SKIPPED", 
                                skip_reason=ManifestStore.BINARY,
                                mtime_epoch_ms=mtime_ms,
                                indexed_at_epoch_ms=indexed_at_ms
                            )
                            skipped_files += 1
                            continue
                except Exception as e:
                    logger.warning("Error checking for binary file %s: %s", file_path, e)
                    # If we can't read it, we might want to skip it or handle it as an error
                    # For now, following the requirement to not crash
                    manifest.add_entry(scan_rules.normalize_root_path(file_path), status="SKIPPED", skip_reason="READ_ERROR")
                    skipped_files += 1
                    continue

                indexed_files += 1
                increment_indexed_files(repo_id)

                # UTF-8 decoding with fallback
                try:
                    with open(file_path, "rb") as f_bytes:
                        content_bytes = f_bytes.read()
                        mtime_ms = int(os.path.getmtime(file_path) * 1000)
                        indexed_at_ms = int(time.time() * 1000)
                        try:
                            # Attempt normal UTF-8 decode
                            content_bytes.decode("utf-8")
                            manifest.add_entry(
                                scan_rules.normalize_root_path(file_path), 
                                status="INDEXED",
                                mtime_epoch_ms=mtime_ms,
                                indexed_at_epoch_ms=indexed_at_ms
                            )
                        except UnicodeDecodeError:
                            # Fallback to replace on error
                            content_bytes.decode("utf-8", errors="replace")
                            manifest.add_entry(
                                scan_rules.normalize_root_path(file_path), 
                                status="INDEXED", 
                                encoding="utf-8-replace",
                                mtime_epoch_ms=mtime_ms,
                                indexed_at_epoch_ms=indexed_at_ms
                            )
                except Exception as e:
                    logger.warning("Error reading file content %s: %s", file_path, e)
                    # If we already incremented indexed_files but now fail to read, 
                    # we might want to adjust, but following the "must not crash" rule.
                    continue

    # Save manifest
    manifest.save()

    duration_ms = int((time.time() - start_time) * 1000)

    # Return results including manifest_path
    return {
        "repo_id": repo_id,
        "mode": "full",
        "indexed_files": indexed_files,
        "skipped_files": skipped_files,
        "chunks_added": 0,  # Placeholder until chunking is integrated
        "duration_ms": duration_ms,
        "manifest_path": str(manifest_path),
    }
